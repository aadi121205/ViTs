{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PBU_ln9BtTwG",
    "outputId": "5fe3d946-0d27-4c27-dfcd-946bdbc68160"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive',force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AWyo0osetX3b",
    "outputId": "20f96e69-6248-4ef0-9098-3fccd872bf0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
      "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
      "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
      "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
      "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
      "\n",
      "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
      "Enter your token (input will not be visible): \n",
      "Add token as git credential? (Y/n) n\n",
      "Token is valid (permission: fineGrained).\n",
      "The token `forme` has been saved to /root/.cache/huggingface/stored_tokens\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful.\n",
      "The current active token is: `forme`\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "i0X3JRbTzBpI"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import SiglipProcessor, SiglipModel, SiglipVisionModel, SiglipImageProcessor,AutoModel,AutoImageProcessor,SiglipConfig, Gemma3ImageProcessor\n",
    "import torch\n",
    "import inspect\n",
    "import matplotlib.pyplot as plt\n",
    "from peft import PeftModel\n",
    "from transformers import AutoProcessor, AutoModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "uKYM1lrSy36A"
   },
   "outputs": [],
   "source": [
    "url = \"/content/drive/MyDrive/images/0.jpg\"\n",
    "image = Image.open(url)\n",
    "resize = image.resize((896, 896))\n",
    "width, height = resize.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296,
     "referenced_widgets": [
      "c35b42a6940844c3b38665085b7ea89f",
      "015e62f4951841e5bbdd5543e2549798",
      "f7bd9c23d8a3471090f8bddd43f89f6e",
      "60f1db85218843c7b2de2e54546603fa",
      "2ee1925f42a04496a1cbfd917e805ca8",
      "76dfc5c2993f42a58fcbf6408e847748",
      "85b4d91c8e394c06a527be04e57cc3ab",
      "04ca17d9634345139d2d4b3081ac3e5a",
      "0d54c7ca52934a9b8cd938a889f26f3f",
      "5f5a2858d82e4074ad8df684ce11e0f4",
      "63684e9ba5464b478aedc6000f199523",
      "a013958e40c94d37ab704a916a89126d",
      "71e5cf9f182c4b53bfa05857035a2759",
      "a819521200ef4376b7242d80637258b4",
      "c346655bd7744fc0914b7683c7f77cd0",
      "340e801e6c104f3d80e68fd4386fd437",
      "d61a3a7358cb416fbefadcee4a3bbacc",
      "35f7b9487a1447b9adfe574e919e7c08",
      "24947c962d2d453f8dd597de6fcccbb9",
      "cd4599e6209e42cdabae0049839991c1",
      "1cbbcb1e756248fe812429bb76b19ec4",
      "d96654c402c947d69f4d81683f454c4c",
      "d3e214b5991d47d3a7061d629315032c",
      "2baffbbfffba4aeeb7335031c93e6835",
      "2dd13b7423d24bef80721caec6eb4c04",
      "69464e2b20094823a008f26bbb73dcbb",
      "13ad9deb2f8c4ba9b13e4e2e36d2e35f",
      "2a771a85ca31455caca2e47b0b61462b",
      "9e8a755482b3432e84fb1aef94679ab6",
      "af3ce3a96f3541049fcacf8e542000e0",
      "c1c3338911d045709d0bc6177b4883d5",
      "1d99d07a286842d991123ad13b77c6b1",
      "3ea65c0f03464832b532e05cf59f22be",
      "2ddd826de43443a68ecc4587975cc6d3",
      "7c531dfdce6a432b828f0351b78dce42",
      "10f9bf9db2c4450b9d60596f06d87583",
      "ca9c36fb0f0c45599ecaf5762ffb3fad",
      "6555bfe795544a57a3107b3082fcdd07",
      "4ce686c39df3409fad13310161951983",
      "7b5b48048e1e4d03b170fc57cffbc6f2",
      "3b2a3fec14b84f6b9b7d81d56653f124",
      "a6439415b1f84d1c81cde811ce3b7e18",
      "63f03e2e561445f58b67001c517d5662",
      "bc71ac5d8ae44f628e63545e02b3f577",
      "00a78e2528a845639a9121240c6178df",
      "067f379367894b65ae886031f0863634",
      "cf1a076fab1a4cfb8478bedb7ecc7ede",
      "703dffbc71544eb581487500c1470e14",
      "84e34cde7e1c4c939d508039d0e4a790",
      "2d7b4eccbbe444718c78b1835b008195",
      "3ee03f155f8d450fb31ef3ec1dd27a4a",
      "f04d944aeb564fa4846d4d7bb025339d",
      "133fa9225e4043fd90397773dca7709d",
      "0eda9a5c2e764c629168755886b58516",
      "54e92c4f1e494b7da6c572b517408666",
      "cfebeba8078f4338bc50f445753cff8a",
      "f9c5aa211b73408d95a6a9b39f85465c",
      "bdcd70a17e67405cb7ee8c8e2083c613",
      "ed402e73dc0540178ab4bfe96159455c",
      "2525a50f806c4f35a3fbffb921d05c59",
      "3044d7026c8648b8a3ca33ae81fa2688",
      "b66719d078804a5785b0e267747d17f7",
      "f834daca861748ffa4d18bf7888e1e76",
      "1f284c76cab74d9785ace29db2736ebf",
      "106478038a7b4789aae91aa351ffb2c0",
      "64d06a33384f49098429a15fc1541de8",
      "691041aeaba34a45b09c625cdfb4e877",
      "e740e59af8fe416d928c21e041e99bd0",
      "8b62f88043fd47b5bf5380e5d3f4061a",
      "1038b018cebf4f098ecd62a65f1333f2",
      "20acd21bf8264510a51459b59aa9cf9b",
      "e3c73de76420459b9d1791546336ba7b",
      "e1a1da847f2b4be58997c84a305ad8c5",
      "13709cbaaff14417a4f4a7cbb9705dd2",
      "96aa9cacee3e4c7f9bf1701b73f602d6",
      "c715a874d9f2483c91462047284472c8",
      "d93fc2fc919848cf84c81275f2672705"
     ]
    },
    "id": "HM0LdIa9tYbO",
    "outputId": "223ef5c0-5576-4d93-a398-dcb233a30d4d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c35b42a6940844c3b38665085b7ea89f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/368 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a013958e40c94d37ab704a916a89126d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/711 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3e214b5991d47d3a7061d629315032c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ddd826de43443a68ecc4587975cc6d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/409 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00a78e2528a845639a9121240c6178df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.40M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfebeba8078f4338bc50f445753cff8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/432 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "691041aeaba34a45b09c625cdfb4e877",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/813M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+\n"
     ]
    }
   ],
   "source": [
    "# Load model and processor\n",
    "processor = SiglipProcessor.from_pretrained(\"google/siglip-base-patch16-224\")\n",
    "model = SiglipModel.from_pretrained(\"google/siglip-base-patch16-224\")\n",
    "\n",
    "url = \"/content/drive/MyDrive/images/0.jpg\"\n",
    "\n",
    "# Your image and text\n",
    "image = Image.open(url).convert(\"RGB\")\n",
    "text = \"a description of the image\"\n",
    "inputs = processor(images=image, text=text, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model(**inputs)\n",
    "if outputs != None:\n",
    "  print(\"+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c18BV6LZ2ixR",
    "outputId": "ebbfca43-1085-4a0f-e91d-fdd203926e2c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+\n"
     ]
    }
   ],
   "source": [
    "processor = SiglipImageProcessor.from_pretrained(\"google/siglip-base-patch16-224\")\n",
    "model = SiglipVisionModel.from_pretrained(\"google/siglip-base-patch16-224\")\n",
    "\n",
    "image = Image.open(url).convert(\"RGB\")\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model(**inputs)\n",
    "if outputs != None:\n",
    "  print(\"+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "-zW--Po4yzUt"
   },
   "outputs": [],
   "source": [
    "_VISION_CONFIG = {\n",
    "    \"hidden_size\": 1152,\n",
    "    \"intermediate_size\": 4304,\n",
    "    \"num_hidden_layers\": 27,\n",
    "    \"num_attention_heads\": 16,\n",
    "    \"num_channels\": 3,\n",
    "    \"image_size\": 896,\n",
    "    \"patch_size\": 14,\n",
    "    \"hidden_act\": \"gelu_pytorch_tanh\",\n",
    "    \"layer_norm_eps\": 1e-6,\n",
    "    \"attention_dropout\": 0.0,\n",
    "    \"vision_use_head\": False,\n",
    "}\n",
    "vision_config=SiglipConfig(**_VISION_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xhG136Z7zQLe",
    "outputId": "f90108c0-d307-43b9-f670-4a23542dcfa4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SiglipVisionTransformer(\n",
      "  (embeddings): SiglipVisionEmbeddings(\n",
      "    (patch_embedding): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), padding=valid)\n",
      "    (position_embedding): Embedding(196, 768)\n",
      "  )\n",
      "  (encoder): SiglipEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-11): 12 x SiglipEncoderLayer(\n",
      "        (layer_norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attn): SiglipAttention(\n",
      "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (layer_norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): SiglipMLP(\n",
      "          (activation_fn): PytorchGELUTanh()\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (post_layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (head): SiglipMultiheadAttentionPoolingHead(\n",
      "    (attention): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "    )\n",
      "    (layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (mlp): SiglipMLP(\n",
      "      (activation_fn): PytorchGELUTanh()\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#model = AutoModel.from_config(config=vision_config)\n",
    "model = SiglipModel(vision_config).vision_model.eval()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 273,
     "referenced_widgets": [
      "577933ef115d45238fb52315a374247a",
      "f5b738043ead46cfad09fbf0dd97d8dd",
      "6daa1fe3a5aa48f7846ad7e0d29fcc5d",
      "9d1b3a32132d42f2bb32a2a0e63c0546",
      "bd3a8f1828e84c3bb6d23bd3aaebf6c9",
      "6f47227c05744f0291bc16e89ce7ab08",
      "9f6d10a539cb4d798ba40fa066473143",
      "80f9c268b96a404e8a6f4ad7a869d2a1",
      "f169ea32a62647c58ea1ad753e9cfb82",
      "b33aab39c3db4b3cbdcb12706a39c7d8",
      "e25c5c96494149fcba0b9c6d15b84e9a",
      "410d6c38dded4427bbd77a262703a733",
      "4b97a0d56f7a49ca89fb9e74e5954cfe",
      "ae034d13117144bbaaced8c8bc7da0f1",
      "fea7619db8194b47952186593c245ad1",
      "0a0f8fe661024186a32e5679b9bec950",
      "3a0cd59a49f549e3ab96e39523ba6888",
      "9ebb926ae44c4f1d95e2ca0de9a56d86",
      "1724da06ac37436b9e08c561a908b7c2",
      "11d48f6e59964247b496fc2dade811f3",
      "031809d18b9040c9a28caf0649e92648",
      "5d1206add7574c0e955a7eb28ff3319a",
      "fc46f3a374674744965c28508f482c6d",
      "7aaa5a45d95f4115823f2b31a2e4d9f4",
      "b72ceeb1f76b422a89ea779f9a7d5e3e",
      "3845353243f94af29b2ff3cf9b2eff2b",
      "72f981d227124889adb00c7ebccf58ba",
      "7f31d53814d8494faf21d25af346cf23",
      "da424f208b8e49c79e56a22a7c5f8c27",
      "166c6fcde212479696f83d16eef48ac1",
      "5f716ec29f1542a4ab926d37bd369755",
      "a8d0c6ca75644f5ab850964f8e0e79c8",
      "473342522c494ca9a8c476706571096d",
      "b72b33907586430e8501044d4cc5e45c",
      "f6109015e424464881e3e714b61f349f",
      "c7ca9d6c5ae9495dadf0c9760a837a1f",
      "818855beb207423fb10bac1832398ecf",
      "f96a4c2bc6374a7fb3437bddc1642448",
      "eb1fb0617db8497ea0d041cb9dc9cd8d",
      "d9962189a6ee41889da741f1dd91b5bc",
      "853bee2a24454ebfad5aee66c0b51ddc",
      "cc9b364aff8a459c96cc62b89a8d241c",
      "52241380b51044ca86ccb75e9dd29a86",
      "f12fd1c426a343a0a4182ce25f6b63d2",
      "a68ae5c75240415abeacc3ac4505d2de",
      "ca3223c628674fb78d9bb6b8c5c41cf7",
      "efa287ad56de4db5a4ad618d9bb7422f",
      "f0de6564db8447d482af657e75ed6449",
      "78a0dafd0120497882c11da629aba4fe",
      "8af4651fd5ef4e928c9bcc614f604496",
      "504ae70db9244779b9124e7ec0199578",
      "283bb7daa01a4f34aabaf57bad516577",
      "7105b1ea01cb4f6a874f0bd6f8075dab",
      "bbb6091ad96f407190658cb38ac13c8c",
      "791d743b11ed439c8daa519811264b2a",
      "961baec58c164b82a940672810adc4b1",
      "588ce2d6ffcd47fcb1d51602eeb0eaff",
      "0de42b34b81a4c61b082a6891e0bbc21",
      "ccf3785150d54ccdb22cbeffe1417ae0",
      "56059b7b2f5a43ebab96f6d65a164681",
      "d94ff0024e2d4e4eb4c7eb08c23b11ce",
      "f1d86ab0119e4664bf37fff3a08f2b35",
      "3f17cee2e003480aa15ceb78cc04ab34",
      "3fc017e11dce4f949cd548f81c089912",
      "a35af444d02c46e9a129db2dc3dc2dd4",
      "2415eaf64124402297f5f854b784a3e1",
      "e3aacdc62f274056970a30a452df3efa",
      "f484eac9a37a44acbc3629b0e109f852",
      "b12eaeb3622b4f81ae6bee87519d2e9f",
      "4764dae49d3d47b1a461fe88298ae374",
      "1a2271dd48fb42ddb718ca2cd53e439c",
      "2a2d08c6aac643b2bc34c1aa2156c766",
      "8f4f42bb7bff46128b1769ca2d10cdae",
      "f3fc651896604aff9fde78ac5274587b",
      "b5e2487eec904a38890eb3712d896e30",
      "a380e72db0f946f6a93b63cbf368bf6f",
      "440cca0b75f14fe185f90f33042388b3",
      "7971bda27542404199589ae294328051",
      "a0b6995435df41eb8cbc8c1e6703feda",
      "4f63563c66e04a8ca2edaafb194778fe",
      "1b96dc9448404fdf873e3df9f0c1b50e",
      "7e59df5f87d34c28901d9b2bb9a2fd57",
      "375ac7bf09d44cc0a9ca2c1bc3ee07fa",
      "fab5d558d749477d90fa09b705d584cc",
      "a10adb53c4c94c65959ff99b92d36907",
      "1f9aa8e9a30a4d6a9bd4e60ec038b976",
      "8c7f7000c5c34459bff0bdae82595fdc",
      "bf716304761f451db8320c36e48538bf"
     ]
    },
    "id": "C5UlPhVs3rWO",
    "outputId": "2e67912e-fe43-4f06-a7a6-8ff3e8639338"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "577933ef115d45238fb52315a374247a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "processor_config.json:   0%|          | 0.00/70.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "410d6c38dded4427bbd77a262703a733",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.json:   0%|          | 0.00/1.61k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc46f3a374674744965c28508f482c6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b72b33907586430e8501044d4cc5e45c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.16M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a68ae5c75240415abeacc3ac4505d2de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.69M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "961baec58c164b82a940672810adc4b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3aacdc62f274056970a30a452df3efa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/35.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7971bda27542404199589ae294328051",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/662 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = \"google/gemma-3-4b-it\"\n",
    "\n",
    "processor1 = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "members1 = dict(inspect.getmembers(processor1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RAHI-yIp-xM1",
    "outputId": "45f81e01-ddf6-4c54-9d63-4f2d3fa33368"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['__annotations__', '__call__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_auto_class', '_create_repo', '_get_arguments_from_pretrained', '_get_files_timestamps', '_merge_kwargs', '_process_messages_for_chat_template', '_upload_modified_files', 'apply_chat_template', 'attributes', 'batch_decode', 'boi_token', 'chat_template', 'decode', 'feature_extractor_class', 'from_args_and_dict', 'from_pretrained', 'full_image_sequence', 'get_possibly_dynamic_module', 'get_processor_dict', 'image_processor', 'image_processor_class', 'image_seq_length', 'image_token', 'image_token_id', 'model_input_names', 'optional_attributes', 'optional_call_args', 'post_process_image_text_to_text', 'prepare_and_validate_optional_call_args', 'push_to_hub', 'register_for_auto_class', 'save_pretrained', 'to_dict', 'to_json_file', 'to_json_string', 'tokenizer', 'tokenizer_class', 'valid_kwargs', 'validate_init_kwargs'])\n"
     ]
    }
   ],
   "source": [
    "print(members1.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "--YIUrqy_K4w",
    "outputId": "d6f5a39a-87a4-4e8c-fe08-fdd0e94bbc64"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_processor: attribute / data = Gemma3ImageProcessor {\n",
      "  \"do_convert_rgb\": null,\n",
      "  \"do_normalize\": true,\n",
      "  \"do_pan_and_scan\": null,\n",
      "  \"do_rescale\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"image_mean\": [\n",
      "    0.5,\n",
      "    0.5,\n",
      "    0.5\n",
      "  ],\n",
      "  \"image_processor_type\": \"Gemma3ImageProcessor\",\n",
      "  \"image_seq_length\": 256,\n",
      "  \"image_std\": [\n",
      "    0.5,\n",
      "    0.5,\n",
      "    0.5\n",
      "  ],\n",
      "  \"pan_and_scan_max_num_crops\": null,\n",
      "  \"pan_and_scan_min_crop_size\": null,\n",
      "  \"pan_and_scan_min_ratio_to_activate\": null,\n",
      "  \"processor_class\": \"Gemma3Processor\",\n",
      "  \"resample\": 2,\n",
      "  \"rescale_factor\": 0.00392156862745098,\n",
      "  \"size\": {\n",
      "    \"height\": 896,\n",
      "    \"width\": 896\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "\n",
    "for name, member in members1.items():\n",
    "\n",
    "    if inspect.ismethod(member):\n",
    "        kind = \"method\"\n",
    "    elif inspect.isfunction(member):\n",
    "        kind = \"function\"\n",
    "    elif inspect.isbuiltin(member):\n",
    "        kind = \"builtin function\"\n",
    "    elif inspect.isclass(member):\n",
    "        kind = \"class\"\n",
    "    elif isinstance(member, property):\n",
    "        kind = \"property\"\n",
    "    elif inspect.ismodule(member):\n",
    "        kind = \"module\"\n",
    "    elif inspect.isroutine(member):\n",
    "        kind = \"routine\"\n",
    "    elif inspect.isdatadescriptor(member):\n",
    "        kind = \"data descriptor\"\n",
    "    else:\n",
    "        kind = \"attribute / data\"\n",
    "\n",
    "    if name == \"image_processor\":\n",
    "      print(f\"{name}: {kind} = {repr(member)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DCBIvpMWA_qt",
    "outputId": "312e459d-6a3c-48de-cdc3-6c395ac77854"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemma3ImageProcessor {\n",
      "  \"do_convert_rgb\": null,\n",
      "  \"do_normalize\": true,\n",
      "  \"do_pan_and_scan\": null,\n",
      "  \"do_rescale\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"image_mean\": [\n",
      "    0.5,\n",
      "    0.5,\n",
      "    0.5\n",
      "  ],\n",
      "  \"image_processor_type\": \"Gemma3ImageProcessor\",\n",
      "  \"image_seq_length\": 256,\n",
      "  \"image_std\": [\n",
      "    0.5,\n",
      "    0.5,\n",
      "    0.5\n",
      "  ],\n",
      "  \"pan_and_scan_max_num_crops\": null,\n",
      "  \"pan_and_scan_min_crop_size\": null,\n",
      "  \"pan_and_scan_min_ratio_to_activate\": null,\n",
      "  \"processor_class\": \"Gemma3Processor\",\n",
      "  \"resample\": 2,\n",
      "  \"rescale_factor\": 0.00392156862745098,\n",
      "  \"size\": {\n",
      "    \"height\": 896,\n",
      "    \"width\": 896\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "image_processor = processor1.image_processor\n",
    "print(image_processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c8hIlbeOBOX9",
    "outputId": "fdb256b2-e176-48db-e977-2f369d9062c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pixel_values': tensor([[[[-0.5765, -0.6941, -0.8196,  ..., -0.6941, -0.7020, -0.7176],\n",
      "          [-0.6627, -0.7255, -0.6941,  ..., -0.6627, -0.6863, -0.7098],\n",
      "          [-0.7882, -0.7804, -0.5529,  ..., -0.6627, -0.6863, -0.7098],\n",
      "          ...,\n",
      "          [ 0.3412, -0.0353, -0.0745,  ..., -0.3412, -0.4667, -0.5294],\n",
      "          [ 0.2863, -0.0196, -0.1216,  ...,  0.0275, -0.4431, -0.5216],\n",
      "          [ 0.0980,  0.0980,  0.2078,  ...,  0.1765, -0.4510, -0.5843]],\n",
      "\n",
      "         [[-0.5294, -0.6627, -0.7961,  ..., -0.6863, -0.6863, -0.6863],\n",
      "          [-0.6157, -0.6941, -0.6706,  ..., -0.6549, -0.6627, -0.6784],\n",
      "          [-0.7412, -0.7490, -0.5294,  ..., -0.6549, -0.6706, -0.6784],\n",
      "          ...,\n",
      "          [ 0.3961,  0.0118, -0.0431,  ..., -0.2471, -0.3804, -0.4667],\n",
      "          [ 0.3490,  0.0353, -0.0902,  ...,  0.1294, -0.3569, -0.4588],\n",
      "          [ 0.1608,  0.1451,  0.2471,  ...,  0.2784, -0.3569, -0.5137]],\n",
      "\n",
      "         [[-0.4039, -0.5843, -0.7490,  ..., -0.6471, -0.6549, -0.6627],\n",
      "          [-0.4980, -0.6235, -0.6235,  ..., -0.6157, -0.6314, -0.6549],\n",
      "          [-0.6235, -0.6784, -0.4824,  ..., -0.6157, -0.6392, -0.6549],\n",
      "          ...,\n",
      "          [-0.0353, -0.4275, -0.4275,  ..., -0.5608, -0.6863, -0.7804],\n",
      "          [-0.1059, -0.4275, -0.4667,  ..., -0.2078, -0.6627, -0.7647],\n",
      "          [-0.2784, -0.3176, -0.1294,  ..., -0.0588, -0.6706, -0.8196]]]])}\n"
     ]
    }
   ],
   "source": [
    "#inputs = image_processor(images=image, return_tensors=\"pt\")\n",
    "inputs = image_processor(images=image, return_tensors=\"pt\", size={\"height\": 224, \"width\": 224})\n",
    "# Remove unsupported keys before calling the model\n",
    "if 'num_crops' in inputs:\n",
    "    del inputs['num_crops']\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2eoHjvkN5CRH",
    "outputId": "00b4eca6-1d96-4fd5-ae39-1bcf833f489b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['last_hidden_state', 'pooler_output'])\n",
      "torch.Size([1, 196, 768])\n",
      "torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "#outputs = model(**inputs)\n",
    "outputs = model(pixel_values=inputs[\"pixel_values\"])\n",
    "print(outputs.keys())\n",
    "print(outputs['last_hidden_state'].size())\n",
    "print(outputs['pooler_output'].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GlJ-HIhRSIUh",
    "outputId": "a2afdbca-4425-4706-f0cd-17d76484ab68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.14.0)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.1)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.5.2)\n",
      "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.45.5)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from peft) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from peft) (24.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from peft) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft) (2.6.0+cu124)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from peft) (4.67.1)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from peft) (0.5.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from peft) (0.30.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.0->peft) (2025.3.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.0->peft) (4.13.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install peft transformers accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 599
    },
    "id": "zCrDs-uVTVWY",
    "outputId": "18bd8016-be22-45de-98cb-82df300aba91"
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "google/gemma-3-vision-tower is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    408\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://huggingface.co/google/gemma-3-vision-tower/resolve/main/config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_files\u001b[0;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    423\u001b[0m             \u001b[0;31m# This is slightly better for only 1 file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m             hf_hub_download(\n\u001b[0m\u001b[1;32m    425\u001b[0m                 \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m    960\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 961\u001b[0;31m         return _hf_hub_download_to_cache_dir(\n\u001b[0m\u001b[1;32m    962\u001b[0m             \u001b[0;31m# Destination\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1067\u001b[0m         \u001b[0;31m# Otherwise, raise appropriate error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1068\u001b[0;31m         \u001b[0m_raise_on_head_call_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_call_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_download\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1069\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_raise_on_head_call_error\u001b[0;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[1;32m   1595\u001b[0m         \u001b[0;31m# Unauthorized => likely a token issue => let's raise the actual error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1596\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mhead_call_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1597\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1483\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1484\u001b[0;31m                 metadata = get_hf_file_metadata(\n\u001b[0m\u001b[1;32m   1485\u001b[0m                     \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0metag_timeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[1;32m   1400\u001b[0m     \u001b[0;31m# Retrieve metadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1401\u001b[0;31m     r = _request_wrapper(\n\u001b[0m\u001b[1;32m   1402\u001b[0m         \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"HEAD\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfollow_relative_redirects\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m         response = _request_wrapper(\n\u001b[0m\u001b[1;32m    286\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    308\u001b[0m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m     \u001b[0mhf_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    458\u001b[0m             )\n\u001b[0;32m--> 459\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0m_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRepositoryNotFoundError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRepositoryNotFoundError\u001b[0m: 404 Client Error. (Request ID: Root=1-680220f4-4644631f3e86e95774011e42;827fe9d6-b6f1-4d7e-9849-9b88c4ccab89)\n\nRepository Not Found for url: https://huggingface.co/google/gemma-3-vision-tower/resolve/main/config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated. For more details, see https://huggingface.co/docs/huggingface_hub/authentication",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-46858d6551c5>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Load base Gemma 3 vision model (ensure vision_config is passed correctly)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"google/gemma-3-vision-tower\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoProcessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"google/gemma-3-vision-tower\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    490\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m                 \u001b[0;31m# We make a call to the config file first (which may be absent) to get the commit hash as soon as possible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 492\u001b[0;31m                 resolved_config_file = cached_file(\n\u001b[0m\u001b[1;32m    493\u001b[0m                     \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m                     \u001b[0mCONFIG_NAME\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, **kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \"\"\"\n\u001b[0;32m--> 266\u001b[0;31m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcached_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilenames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_files\u001b[0;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    454\u001b[0m         \u001b[0;31m# We cannot recover from them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRepositoryNotFoundError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGatedRepoError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m             raise OSError(\n\u001b[0m\u001b[1;32m    457\u001b[0m                 \u001b[0;34mf\"{path_or_repo_id} is not a local folder and is not a valid model identifier \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                 \u001b[0;34m\"listed on 'https://huggingface.co/models'\\nIf this is a private repository, make sure to pass a token \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: google/gemma-3-vision-tower is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoProcessor\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "# Load base Gemma 3 vision model (ensure vision_config is passed correctly)\n",
    "model = AutoModel.from_pretrained(\"google/gemma-3-vision-tower\")\n",
    "processor = AutoProcessor.from_pretrained(\"google/gemma-3-vision-tower\")\n",
    "\n",
    "# Freeze base model\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7v_XuQ0iTXOI"
   },
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=16,                                # rank of adaptation\n",
    "    lora_alpha=32,                       # scaling factor\n",
    "    target_modules=[\"q_proj\", \"v_proj\"], # depends on model architecture\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.FEATURE_EXTRACTION  # or TaskType.IMAGE_CLASSIFICATION\n",
    ")\n",
    "lora_model = get_peft_model(model, lora_config)\n",
    "lora_model.print_trainable_parameters()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPDBsxHTIrsGspY8xkr9KEC",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
